# Quick Reference - PadrÃµes de ParalelizaÃ§Ã£o

## ğŸš€ **PadrÃµes Aplicados**

### **1. Worker Pool Pattern**
```go
// Origem: Java ExecutorService, .NET TPL
var wg sync.WaitGroup
for i := 0; i < numWorkers; i++ {
    wg.Add(1)
    go worker(i, &wg)
}
wg.Wait()
```

### **2. Fork-Join Pattern**
```go
// Origem: Java ForkJoinPool, OpenMP parallel sections
// FORK: Launch workers
go worker1()
go worker2()
// JOIN: Wait for completion
wg.Wait()
```

### **3. Data Parallelism Pattern**
```go
// Origem: OpenMP #pragma omp parallel for, Rayon par_iter()
keysPerWorker := (totalKeys + numWorkers - 1) / numWorkers
for i := 0; i < numWorkers; i++ {
    start := i * keysPerWorker
    end := start + keysPerWorker
    go processRange(start, end)
}
```

### **4. Critical Section Pattern**
```go
// Origem: OpenMP #pragma omp critical, mutex patterns
// Sequential operations that cannot be parallelized
for _, item := range criticalData {
    // Must be sequential for correctness
}
```

## ğŸ”§ **PadrÃµes EspecÃ­ficos do Go**

### **Independent Random Streams**
```go
// Problema: Race conditions em geraÃ§Ã£o de nÃºmeros aleatÃ³rios
func (b *ISBenchmark) findMySeed(processorRank, numberProcessor int, ...) float64 {
    // Algoritmo "skip-ahead" do OpenMP
    // Cada worker tem seu prÃ³prio stream independente
}
```

### **Parallel Initialization**
```go
// Problema: InicializaÃ§Ã£o de grandes arrays
func (b *ISBenchmark) allocKeyBuff() {
    // Chunk-based parallel initialization (Rayon pattern)
    var wg sync.WaitGroup
    for i := 0; i < b.numProcs; i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            // Parallel initialization of chunk
        }(i)
    }
    wg.Wait()
}
```

### **Adaptive Load Balancing**
```go
// Problema: DistribuiÃ§Ã£o uniforme de trabalho
keysPerWorker := (NUM_KEYS + b.numProcs - 1) / b.numProcs
k1 := keysPerWorker * workerID
k2 := k1 + keysPerWorker
if k2 > NUM_KEYS {
    k2 = NUM_KEYS
}
```

## ğŸ“Š **ComparaÃ§Ã£o com Outras Linguagens**

| PadrÃ£o | C++ (OpenMP) | Rust (Rayon) | Go (Nativo) | Java (ExecutorService) |
|--------|--------------|--------------|-------------|----------------------|
| **Worker Pool** | `#pragma omp parallel` | `par_iter()` | `go func()` | `ExecutorService.submit()` |
| **Fork-Join** | `#pragma omp sections` | `join()` | `sync.WaitGroup` | `ForkJoinPool` |
| **Data Parallelism** | `#pragma omp parallel for` | `par_chunks()` | Range-based workers | `Parallel.For()` |
| **Critical Section** | `#pragma omp critical` | `mutex` | Sequential code | `synchronized` |

## ğŸ¯ **Quando Usar Cada PadrÃ£o**

### **Worker Pool Pattern**
- âœ… **Use quando**: Controle preciso do nÃºmero de workers
- âœ… **Use quando**: Trabalho independente entre workers
- âŒ **NÃ£o use quando**: Trabalho muito pequeno (overhead)

### **Fork-Join Pattern**
- âœ… **Use quando**: DecomposiÃ§Ã£o natural de problemas
- âœ… **Use quando**: SincronizaÃ§Ã£o automÃ¡tica necessÃ¡ria
- âŒ **NÃ£o use quando**: DependÃªncias complexas entre tasks

### **Data Parallelism Pattern**
- âœ… **Use quando**: Processamento de arrays grandes
- âœ… **Use quando**: Trabalho uniforme por elemento
- âŒ **NÃ£o use quando**: DependÃªncias entre elementos

### **Critical Section Pattern**
- âœ… **Use quando**: OperaÃ§Ãµes que devem ser sequenciais
- âœ… **Use quando**: Garantia de correÃ§Ã£o Ã© crÃ­tica
- âŒ **NÃ£o use quando**: Performance Ã© mais importante que correÃ§Ã£o

## ğŸš€ **OtimizaÃ§Ãµes Go-Specific**

### **sync.Pool para Memory Reuse**
```go
var bufferPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 1024)
    },
}
```

### **Channels para Communication**
```go
results := make(chan Result, numWorkers)
for i := 0; i < numWorkers; i++ {
    go worker(i, results)
}
```

### **Context para Cancellation**
```go
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
defer cancel()
```

## ğŸ“ˆ **MÃ©tricas de Performance**

### **Speedup Esperado**
- **2 cores**: ~1.8x
- **4 cores**: ~3.2x
- **8 cores**: ~5.6x

### **Overhead de SincronizaÃ§Ã£o**
- **WaitGroup**: ~1-2Î¼s
- **Channel**: ~5-10Î¼s
- **Mutex**: ~10-20Î¼s

### **Memory Overhead**
- **Goroutine**: ~2KB stack
- **Channel**: ~96 bytes
- **WaitGroup**: ~12 bytes

## ğŸ” **Debugging Patterns**

### **Worker Identification**
```go
func worker(workerID int, wg *sync.WaitGroup) {
    defer wg.Done()
    log.Printf("Worker %d: starting", workerID)
    // ... work ...
    log.Printf("Worker %d: completed", workerID)
}
```

### **Performance Monitoring**
```go
start := time.Now()
// ... parallel work ...
elapsed := time.Since(start)
log.Printf("Parallel work took %v", elapsed)
```

### **Error Handling**
```go
func worker(workerID int, wg *sync.WaitGroup, errChan chan error) {
    defer wg.Done()
    if err := doWork(); err != nil {
        errChan <- fmt.Errorf("worker %d: %w", workerID, err)
    }
}
```

## ğŸ¯ **Best Practices**

### **1. Start Simple**
- Paralelizar apenas o que Ã© seguro
- Validar correÃ§Ã£o constantemente
- Medir impacto de cada mudanÃ§a

### **2. Use Established Patterns**
- Worker Pool para task distribution
- Fork-Join para decomposiÃ§Ã£o
- Data Parallelism para processamento

### **3. Consider Go-Specific**
- Channels para comunicaÃ§Ã£o
- Context para cancellation
- sync.Pool para memory reuse

### **4. Profile Before Optimize**
- Identificar bottlenecks reais
- Medir overhead de sincronizaÃ§Ã£o
- Validar escalabilidade

---

**Esta referÃªncia rÃ¡pida fornece os padrÃµes essenciais para implementaÃ§Ã£o de paralelizaÃ§Ã£o em Go, baseados em padrÃµes estabelecidos do mercado e adaptados para as caracterÃ­sticas especÃ­ficas da linguagem.**
